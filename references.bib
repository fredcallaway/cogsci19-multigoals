% -----------------------------------------------------------------------
% File: CogSci_Template.bib
% -----------------------------------------------------------------------

% Modified : Eli M. Silk (esilk at pitt.edu)            05/24/2005
% Modified : David Noelle (dnoelle at ucmerced.edu)     11/19/2014
@article{Keramati2011,
author = {Keramati, Mehdi and Dezfouli, Amir and Piray, Payam},
journal = {PLoS Comput Biol},
number = {5},
pages = {e1002055},
publisher = {Public Library of Science},
title = {{Speed/accuracy trade-off between the habitual and the goal-directed processes}},
volume = {7},
year = {2011}
}

@article{MacGregor2001,
  title={Information processing and insight: a process model of performance on the nine-dot and related problems.},
  author={MacGregor, James N and Ormerod, Thomas C and Chronicle, Edward P},
  journal={Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume={27},
  number={1},
  pages={176},
  year={2001},
  publisher={American Psychological Association}
}

@article{Krusche2018,
abstract = {How do people plan ahead when searching for rewards? We investigate planning in a foraging task in which participants search for rewards on an infinite two-dimensional grid. Our results show that their search is best-described by a model which searches approximately 3 steps ahead. Furthermore, participants do not seem to update their beliefs during plan-ning, but rather treat their initial beliefs as given, a strategy sometimes called root-sampling. This planning algorithm cor-responds well with participants' behavior in test problems with restricted movement and varying degrees of information, out-performing more complex models. These results enrich our understanding of adaptive planning in complex environments.},
author = {Krusche, Moritz J F and Schulz, Eric and Guez, Arthur and Speekenbrink, Maarten},
doi = {10.1101/268938},
file = {:Users/fred/Dropbox/Articles/Mendeley/Krusche et al. - 2018 - Adaptive planning in human search.pdf:pdf},
journal = {bioRxiv},
keywords = {algorithms that are used,decision making,forag-,ing,monte carlo sampling,planning,planning as tree-search,reinforcement learning,tree search,tree-search algorithms are search},
pages = {0--5},
title = {{Adaptive planning in human search}},
year = {2018}
}

@Book{NewellSimon1972,
  author =	 {A. Newell and H. A. Simon},
  title =	 {Human problem solving},
  publisher =	 {Prentice-Hall},
  year =	 1972,
  address =	 {Englewood Cliffs, NJ}
}

@article{Keramati2016,
author = {Keramati, Mehdi and Smittenaar, Peter and Dolan, Raymond J and Dayan, Peter},
doi = {10.1073/pnas.1609094113},
title = {{Adaptive integration of habits into depth-limited planning defines a habitual-goal – directed spectrum}},
year = {2016}
}

@article{Campitelli2004,
author = {Campitelli, Guillermo and Gobet, Fernand R.},
issn = {1389-6911},
journal = {Journal of the International Computer Games Association},
number = {1995},
pages = {209--216},
title = {{Adaptive expert decision making: Skilled chessplayers search more and deeper}},
year = {2004}
}

@article{Huys2012,
author = {Huys, Quentin J M and Eshel, Neir and O'Nions, Elizabeth and Sheridan, Luke and Dayan, Peter and Roiser, Jonathan P},
journal = {PLoS Comput Biol},
keywords = {rl},
number = {3},
pages = {e1002410},
publisher = {Public Library of Science},
title = {{Bonsai trees in your head: how the Pavlovian system sculpts goal-directed choices by pruning decision trees}},
volume = {8},
year = {2012}
}

@article{Huys2015,
abstract = {Humans routinely formulate plans in domains so complex that even the most powerful computers are taxed. To do so, they seem to avail themselves of many strategies and heuristics that efficiently simplify, approximate, and hierarchically decompose hard tasks into simpler subtasks. Theoretical and cognitive research has revealed several such strategies; however, little is known about their establishment, interaction, and efficiency. Here, we use model-based behavioral analysis to provide a detailed examination of the performance of human subjects in a moderately deep planning task. We find that subjects exploit the structure of the domain to establish subgoals in a way that achieves a nearly maximal reduction in the cost of computing values of choices, but then combine partial searches with greedy local steps to solve subtasks, and maladaptively prune the decision trees of subtasks in a reflexive manner upon encountering salient losses. Subjects come idiosyncratically to favor particular sequences of actions to achieve subgoals, creating novel complex actions or "options."},
archivePrefix = {arXiv},
arxivId = {arXiv:1408.1149},
author = {Huys, Quentin J M and Lally, N{\'{i}}all and Faulkner, Paul and Eshel, Neir and Seifritz, Erich and Gershman, Samuel J and Dayan, Peter and Roiser, Jonathan P},
doi = {10.1073/pnas.1414219112},
eprint = {arXiv:1408.1149},
file = {:Users/fred/Dropbox/Articles/Mendeley/Huys et al. - 2015 - Interplay of approximate planning strategies.pdf:pdf},
isbn = {1091-6490 (Electronic)$\backslash$r0027-8424 (Linking)},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Humans,Intelligence,Planning Techniques,Stochastic Processes},
number = {10},
pages = {3098--103},
pmid = {25675480},
title = {{Interplay of approximate planning strategies.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4364207{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {112},
year = {2015}
}

@article{Kool2017,
abstract = {Human behavior is sometimes determined by habit, and other times by goal-directed planning. Modern reinforcement learning theories formalize this distinction as a competition between a computationally cheap but inaccurate “model-free” system that gives rise to habits, and a computationally expensive but accurate “model-based” system that implements planning. It is unclear, however, how we choose to allocate control between these systems. Here, we propose that arbitration occurs by comparing each system's costs and benefits. Consistent with this proposal, we report two experiments showing that people increase model-based control when it achieves greater accuracy than model-free control, and especially when the rewards of accurate performance are amplified. In contrast, they are insensitive to reward amplification when model-based and model-free control yield equivalent accuracy. This suggests that humans adaptively balance habitual and planned action guided by a cost-benefit analysis.},
author = {Kool, Wouter and Gershman, Samuel J. and Cushman, Fiery A.},
doi = {10.1177/0956797617708288},
file = {:Users/fred/Dropbox/Articles/Mendeley/Kool, Gershman, Cushman - 2017 - Cost-Benefit Arbitration Between Multiple Reinforcement-Learning Systems.pdf:pdf},
issn = {14679280},
journal = {Psychological Science},
keywords = {cognitive control,decision making,open data,open materials,reinforcement learning},
number = {9},
pages = {1321--1333},
pmid = {28731839},
title = {{Cost-Benefit Arbitration Between Multiple Reinforcement-Learning Systems}},
volume = {28},
year = {2017}
}

@article{Shallice1982,
author = {Timothy Shallice  and Donald Eric Broadbent  and Lawrence Weiskrantz },
title = {Specific impairments of planning},
journal = {Philosophical Transactions of the Royal Society of London. B, Biological Sciences},
volume = {298},
number = {1089},
pages = {199-209},
year = {1982},
doi = {10.1098/rstb.1982.0082},

URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rstb.1982.0082},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rstb.1982.0082},
    abstract = { An information-processing model is outlined that predicts that performance on non-routine tasks can be impaired independently of performance on routine tasks. The model is related to views on frontal lobe functions, particularly those of Luria. Two methods of obtaining more rigorous tests of the model are discussed. One makes use of ideas from artificial intelligence to derive a task heavily loaded on planning abilities. A group of patients with left anterior lesions has a specific deficit on the task. Subsidiary investigations support the inference that this is a planning impairment. }
}



% -----------------------------------------------------------------------
% Document End
% -----------------------------------------------------------------------
